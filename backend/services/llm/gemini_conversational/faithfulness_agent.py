"""
Faithfulness Agent Module

This module implements a sub-agent responsible for evaluating the faithfulness, adherence,
and completeness of the Main Agent's responses.
"""

import logging
import google.generativeai as genai
from typing import Optional
from core.env_config import get_env_variable

logger = logging.getLogger(__name__)

FAITHFULNESS_SYSTEM_PROMPT = """You are a QA Supervisor agent responsible for evaluating the quality of an AI assistant's response.

Your task is to determine if the Assistant's Response is "Faithful" to the User's Query.
"Faithful" means:
1.  **Adherence**: The response directly addresses the user's specific request.
2.  **No Hallucination**: The response does not invent facts or make up information not supported by context (if context is provided) or general knowledge.
3.  **Completeness**: The response is complete and does not cut off abruptly.
4.  **Sensible**: The response makes logical sense.

## CRITICAL APPLICATION CONTEXT
The agent operates in a **UI-based application** with specific workflow patterns. You MUST consider these when evaluating faithfulness:

### Async Generation Tools (Confirmation-Based)
When users request "generate requirements", "generate scenarios", or "generate test cases":
- The agent calls tools like `start_requirement_generation`, `start_scenario_generation`, `start_testcase_generation`
- These tools **trigger a confirmation modal in the UI** and return a status like "confirmation_required"
- The agent SHOULD respond by telling the user that confirmation is needed (e.g., "Click Yes to start generation")
- This is FAITHFUL behavior because the agent correctly initiated the async workflow
- Do NOT flag as unfaithful just because requirements/scenarios aren't displayed inline

### Display Tools (UI Modal-Based)
When users request "show requirements", "show scenarios", "show test cases":
- The agent calls tools like `show_requirements`, `show_scenarios`, `show_testcases`
- These tools **render content in a separate UI modal/panel above the chat**, not inline
- The agent SHOULD respond by confirming the content is displayed (e.g., "Requirements are displayed above")
- This is FAITHFUL behavior because the agent correctly triggered the UI display
- Do NOT flag as unfaithful just because the content isn't in the text response

### Document Display Tools
When users ask to see document content:
- Tools like `show_extracted_text` display PDF/document content in a **UI modal above the chat**
- The agent correctly tells the user the content is displayed above
- This is FAITHFUL behavior

**INPUTS:**
- Conversation Context: [Recent conversation history for context]
- User Query: [The user's latest message]
- Agent Response: [The response generated by the main agent]

**OUTPUT:**
Return a JSON object with the following structure:
```json
{
    "score": <int 0-100>,
    "is_faithful": <bool>,
    "reason": "<string explanation of why the score was given>"
}
```
- A score of 0-70 indicates unfaithful/incomplete.
- A score of 71-100 indicates faithful.

Do not provide explanations outside the JSON. Return only the JSON object.
"""

class FaithfulnessAgent:
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or get_env_variable("GEMINI_API_KEY", "")
        if not self.api_key:
            logger.warning("FaithfulnessAgent initialized without API KEY.")
        
        self.model_name = "gemini-2.5-flash"
        
    def evaluate(self, user_query: str, agent_response: str, llm=None, context: str = None) -> dict:
        """
        Evaluates the agent's response against the user's query.
        
        Args:
            user_query: The user's original query.
            agent_response: The agent's generated response.
            llm: Optional LangChain model instance to use for evaluation.
            context: Optional conversation context for better evaluation.
        
        Returns:
            dict: {"score": int, "is_faithful": bool, "reason": str}
        """
        # Build context section if provided
        context_section = ""
        if context:
            context_section = f"""Conversation Context (recent history):
{context}

"""
        
        prompt = f"""
{context_section}User Query:
{user_query}

Agent Response:
{agent_response}

Evaluation JSON:
"""

        try:
            import json
            import re
            
            result_text = ""
            
            if llm:
                # Use provided LangChain model
                from langchain_core.messages import SystemMessage, HumanMessage
                messages = [
                    SystemMessage(content=FAITHFULNESS_SYSTEM_PROMPT),
                    HumanMessage(content=prompt)
                ]
                response = llm.invoke(messages)
                result_text = response.content.strip() if hasattr(response, 'content') else str(response).strip()
                logger.info(f"Faithfulness Agent Result (Model-Agnostic): {result_text}")
                
            else:
                # Fallback to direct Gemini usage
                if not self.api_key:
                    logger.error("Cannot evaluate: No API Key.")
                    return {"score": 0, "is_faithful": False, "reason": "No API Key configured"}
                
                genai.configure(api_key=self.api_key)
                model = genai.GenerativeModel(
                    model_name=self.model_name,
                    system_instruction=FAITHFULNESS_SYSTEM_PROMPT
                )
                
                response = model.generate_content(
                    prompt,
                    generation_config=genai.types.GenerationConfig(
                        temperature=0.0,
                        response_mime_type="application/json"
                    )
                )
                result_text = response.text.strip()
                logger.info(f"Faithfulness Agent Result: {result_text}")
            
            # Parse JSON
            try:
                # Remove code fences if present
                clean_text = result_text
                fence_match = re.search(r"```json\s*(\{[\s\S]*?\})\s*```", result_text, re.IGNORECASE) or \
                              re.search(r"```\s*(\{[\s\S]*?\})\s*```", result_text, re.IGNORECASE)
                if fence_match:
                    clean_text = fence_match.group(1)
                elif result_text.strip().startswith("{") and result_text.strip().endswith("}"):
                    clean_text = result_text.strip()
                    
                data = json.loads(clean_text)
                return {
                    "score": int(data.get("score", 0)),
                    "is_faithful": bool(data.get("is_faithful", False)),
                    "reason": data.get("reason", "No reason provided")
                }
            except Exception as e:
                logger.warning(f"Failed to parse JSON from Faithfulness Agent: {e}. Raw: {result_text}")
                # Fallback heuristic
                is_faithful = "true" in result_text.lower() or "1" in result_text
                return {
                    "score": 100 if is_faithful else 0,
                    "is_faithful": is_faithful,
                    "reason": "Failed to parse structured reasoning"
                }

        except Exception as e:
            logger.error(f"Error in FaithfulnessAgent evaluation: {e}")
            return {"score": 0, "is_faithful": False, "reason": f"Evaluation error: {str(e)}"}
